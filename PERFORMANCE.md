# Performance

This document captures the reasoning, methodology, and results of performance optimisation work on the transaction engine.

## Benchmark Input: Scenario-Based Generation vs Random Traces

A benchmark is only as meaningful as its input. For the transaction engine, two approaches to generating large benchmark inputs were considered:

### Option A: Random trace generation

Generate N clients with M transactions each, choosing transaction types at random (e.g., 80% deposits, 15% withdrawals, 5% disputes/resolves/chargebacks). This is simple to implement and can produce arbitrarily large inputs.

**Drawback:** The distribution of transaction types and their sequencing is arbitrary. Random traces don't model realistic client behaviour — in practice, disputes follow deposits, chargebacks follow disputes, and clients don't uniformly alternate between every operation. A random generator either avoids stateful types (disputes, chargebacks) for simplicity, or produces many rejected transactions due to invalid sequences (e.g., disputing a nonexistent deposit). Neither pattern reflects how the engine would be used.

### Option B: Scenario-based generation (chosen)

Reuse the same scenario catalog that powers the correctness tests. Each scenario shape encodes a realistic, self-contained client interaction — a deposit followed by a withdrawal, a deposit-dispute-resolve cycle, a chargeback that freezes an account, etc. The benchmark fixture is generated by repeating the full catalog many times (e.g., 2000 repetitions × 29 shapes ≈ 58,000 clients, ~200K transaction rows), with deterministic but varied parameters for each repetition. The downside here is that the scenarios we have at the moment were chosen as a more or less educated guess, without any data suggesting that they were realistic or that the scenario set is representative or real traces. The mix of transaction types may over-represent edge cases (e.g., frozen accounts, double disputes) relative to a real workload.

**Why this is better:**

1. **Realistic sequencing.** Each scenario shape represents a plausible client story with the correct causal ordering — disputes reference prior deposits, chargebacks follow disputes, etc. The engine exercises the same code paths it would in production.

2. **Full transaction-type coverage.** Every shape in the catalog contributes to the benchmark, so all five transaction types and both success and error paths are exercised in proportion to the catalog's composition.

3. **Single abstraction, three uses.** The scenario catalog serves correctness testing (proptest interleaving), E2E regression (representative fixture), and benchmarking. There is no separate "benchmark-only" code to maintain, drift, or become stale.

4. **Path to production-realistic benchmarks.** In a production setting, real transaction traces would be gathered and distilled into representative scenario shapes — capturing actual client behaviour patterns, type distributions, and error rates. These shapes would slot directly into the existing catalog and flow into benchmarks without any infrastructure changes. The scenario-based approach is designed to grow more realistic over time, rather than being a one-off approximation.

