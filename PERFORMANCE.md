# Performance

This document captures the reasoning, methodology, and results of performance optimisation work on the transaction engine.

## Test Environment

All benchmarks were executed on a consumer laptop environment (plugged into power, high-performance profile, isolated background tasks).

* **OS:** Ubuntu 22.04.5 LTS (Kernel: 6.8.0-100-generic)
* **CPU:** Intel(R) Core(TM) i7-8565U CPU @ 1.80GHz (Mobile)
* **Cores:** 4 Physical Cores / 8 Logical Threads (1 NUMA node)
* **RAM:** 16 GB 
* **Rust Toolchain:** `cargo bench` via Criterion (Release profile, optimized)

## Benchmark Input: Scenario-Based Generation vs Random Traces

A benchmark is only as meaningful as its input. For the transaction engine, two approaches to generating large benchmark inputs were considered:

### Option A: Random trace generation

Generate N clients with M transactions each, choosing transaction types at random (e.g., 80% deposits, 15% withdrawals, 5% disputes/resolves/chargebacks). This is simple to implement and can produce arbitrarily large inputs.

**Drawback:** The distribution of transaction types and their sequencing is arbitrary. Random traces don't model realistic client behaviour — in practice, disputes follow deposits, chargebacks follow disputes, and clients don't uniformly alternate between every operation. A random generator either avoids stateful types (disputes, chargebacks) for simplicity, or produces many rejected transactions due to invalid sequences (e.g., disputing a nonexistent deposit). Neither pattern reflects how the engine would be used.

### Option B: Scenario-based generation (chosen)

Reuse the same scenario catalog that powers the correctness tests. Each scenario shape encodes a realistic, self-contained client interaction — a deposit followed by a withdrawal, a deposit-dispute-resolve cycle, a chargeback that freezes an account, etc. The benchmark fixture is generated by repeating the full catalog many times (e.g., 2000 repetitions × 29 shapes ≈ 58,000 clients, ~200K transaction rows), with deterministic but varied parameters for each repetition. The downside here is that the scenarios we have at the moment were chosen as a more or less educated guess, without any data suggesting that they were realistic or that the scenario set is representative or real traces. The mix of transaction types may over-represent edge cases (e.g., frozen accounts, double disputes) relative to a real workload.

**Why this is better:**

1. **Realistic sequencing.** Each scenario shape represents a plausible client story with the correct causal ordering — disputes reference prior deposits, chargebacks follow disputes, etc. The engine exercises the same code paths it would in production.

2. **Full transaction-type coverage.** Every shape in the catalog contributes to the benchmark, so all five transaction types and both success and error paths are exercised in proportion to the catalog's composition.

3. **Single abstraction, three uses.** The scenario catalog serves correctness testing (proptest interleaving), E2E regression (representative fixture), and benchmarking. There is no separate "benchmark-only" code to maintain, drift, or become stale.

4. **Path to production-realistic benchmarks.** In a production setting, real transaction traces would be gathered and distilled into representative scenario shapes — capturing actual client behaviour patterns, type distributions, and error rates. These shapes would slot directly into the existing catalog and flow into benchmarks without any infrastructure changes. The scenario-based approach is designed to grow more realistic over time, rather than being a one-off approximation.

## Sequential vs Parallel: Motivation and Results

### Why a parallel implementation?

The engine exposes two public APIs: `process()` (sequential) and `process_parallel()` (multi-threaded, client-sharded). The parallel variant was built to explore whether distributing transaction processing across worker threads could improve throughput for large batch inputs.

The architecture uses client-sharding: the main thread parses the CSV and dispatches each transaction to a worker thread based on `client_id % num_workers`. All transactions for a given client land on the same worker, preserving per-client ordering without locks on shared state. Two additional threads handle the `on_success` and `on_error` callbacks. All inter-thread communication uses bounded `sync_channel`s for backpressure.

### Benchmark results

The benchmark uses a fixture of ~176,000 transactions generated from the scenario catalog (29 shapes × 2,000 repetitions). Both modes use no-op callbacks to isolate engine throughput from callback cost.

| Mode | Time | Throughput |
|---|---|---|
| Sequential | ~180 ms | ~972 Kelem/s |
| Parallel (7 workers) | ~636 ms | ~277 Kelem/s |

The sequential API is **~3.5× faster**.

### Reproducing the benchmark

1. **Generate the benchmark fixture** (one-time):

```bash
cargo nextest run --run-ignored only generate_benchmark_fixture
```

This writes `tests/data/benchmark.csv` (~176K rows).

2. **Run the benchmark:**

```bash
cargo bench
```

3. **View the HTML report** (requires a browser):

```
open target/criterion/process/report/index.html
```

The Criterion report shows both modes on the same chart for direct comparison, including confidence intervals and change detection across runs.

### Analysis: why parallel is slower

The bottleneck is **CSV parsing**, which is single-threaded in both modes — the `csv` crate's iterator must be consumed sequentially. The actual per-transaction work (a `HashMap` lookup and some `Decimal` arithmetic) costs on the order of nanoseconds.

The parallel implementation adds two `sync_channel` round-trips per transaction:
1. Main thread → worker (dispatch)
2. Worker → callback thread (result)

Each channel operation involves a mutex acquisition, potential condvar signal, and cache-line invalidation across cores. At ~1–2 μs per hop, 352,000 channel operations account for the ~450 ms overhead observed.

In other words: **the synchronisation cost of coordinating threads is orders of magnitude larger than the work being distributed**. The workers are essentially idle, waiting on channels, while the main thread spends most of its time parsing CSV rows.

### Conclusion

This result directly mirrors the design principles behind high-frequency trading (HFT) systems, where lock-freedom and minimal cross-thread coordination are treated as non-negotiable constraints. Parallelism only pays when the per-item computation significantly exceeds the coordination cost. For this engine's workload — lightweight in-memory state updates fed by a sequential CSV parser — single-threaded processing is the optimal choice.

### When would the parallel mode be beneficial?

The parallel API (`process_parallel`) would outperform the sequential variant in scenarios where the per-transaction processing cost is substantially higher:

- **Heavy callback work** — if `on_success` or `on_error` perform I/O (e.g., writing to a database, publishing to a message broker), the dedicated callback threads prevent the processing pipeline from stalling.
- **Complex domain logic** — if transaction processing involved cryptographic verification, model evaluation, or network lookups, the channel overhead would become negligible relative to the per-item cost.
- **Batched dispatch** — sending chunks of transactions per channel message (instead of one at a time) would amortise synchronisation cost, making the parallel architecture viable even for lighter workloads. This optimisation was not pursued in the current implementation.

## Parser Optimization: Eliminating Heap Allocations

To further improve the throughput of the engine, the parsing of the raw transactions was optimized. Originally, the `csv` parser allocated and immediately dropped a heap `String` for every transaction's `type` field in order to match it against known keywords. 

By refactoring the intermediate representation to leverage Serde's direct byte-to-enum mapping, this per-row heap allocation was entirely eliminated, achieving zero-copy parsing for the transaction type.

The Criterion benchmarks yielded the following improvements across ~176,000 rows:

* **Sequential Mode:** Processing time dropped from ~186.5 ms to ~159.0 ms. This represents a **~15% throughput increase**, pushing the engine past the 1.1 million elements/second threshold.
* **Parallel Mode (7 workers):** Processing time dropped from ~624.1 ms to ~606.7 ms, representing only a **~3-4% throughput increase**.

**Analysis:**
This optimization further reinforces the architectural conclusions drawn above. Because the CSV parser operates sequentially on the main thread in both architectures, the absolute time saved by removing the memory allocation is nearly identical (~25–27 ms) in both modes. 

However, the proportional impact differs drastically. In the sequential mode, saving 27 ms on an already highly optimized 186 ms pipeline yields a massive 15% performance gain. In the parallel mode, that same 27 ms of saved parser time is drowned out by the ~440 ms of channel synchronization overhead.

## Experiment: CPU Core Affinity (Thread Pinning)

To determine if operating system context switching and L1/L2 cache thrashing were introducing hidden overhead to the sequential engine, an experiment was conducted using CPU core pinning via the `core_affinity` crate. The main execution thread was explicitly locked to a single, isolated hardware core to bypass the OS scheduler. To ensure accurate telemetry, the benchmarks were controlled for thermal throttling, and background hardware interrupts were mitigated by targeting the least active core. 

The Criterion results demonstrated a statistically insignificant variance (`p = 0.06 > 0.05`) between the pinned and unpinned executions, with throughput remaining flat at ~1.13 million elements/second. This result clearly demonstrates that the throughput of the single-threaded engine does not profit from explicit core pinning. Given the lack of a measurable performance gain, we opted against introducing OS-specific CPU affinity logic into the `main` application.

## Flamegraph Optimization: Bypassing Intermediate Float Allocations during Deserialization

Profiling the sequential execution using `cargo-flamegraph` revealed a significant CPU bottleneck during the CSV reading phase, specifically centralized around `malloc` and `to_string` calls. The root cause was traced to the default deserialization behavior between the `csv` and `rust_decimal` crates. By default, the `csv` crate parsed transaction amounts as `f64` floats. To safely convert these into precise `Decimal` types without data loss, `rust_decimal` defensively allocated heap memory to convert the float back to a string before parsing it, resulting in a hidden `malloc` and `drop` per row.

To resolve this, we explicitly bypassed the `f64` intermediate step by applying the `#[serde(with = "rust_decimal::serde::str_option")]` attribute to the amount field in the raw transaction struct. This forced the deserializer to parse the `Decimal` directly from the raw string bytes. This zero-allocation fix eliminated 175,000 heap operations per run, yielding a statistically significant 9.5% increase in throughput and dropping execution time from ~155ms to ~142ms.